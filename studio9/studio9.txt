1) kenichi matsuo

2) it occupies all of the cores because we distribute all of the work among all 4 cores
since we parallelised the work. the work load is significant so 4 cores can be used to the max

3)
// 500 million iterations should take several seconds to run
#define ITERS 500000000
#define locked  1;
#define unlocked  0;
int iter_lock;

void critical_section( void ){
	int index = 0;
	while(index < ITERS){ index++; }
}

void lock(volatile int * iter_lock){
	printf("iter_lock: %d\n",*iter_lock);
}

void unlock(volatile int * iter_lock){

}

int main (int argc, char* argv[]){
	iter_lock = unlocked;
	// Create a team of threads on each processor
	#pragma omp parallel
	{
		// Each thread executes this code block independently
		lock(&iter_lock);
		critical_section();
		unlock(&iter_lock);




4)
void lock(volatile int * iter_lock){
	int expected = unlocked;
	while(!__atomic_compare_exchange(&iter_lock, &expected, locked, 0, __ATOMIC_ACQ_REL, __ATOMIC_ACQUIRE)){
		expected = unlocked;
	}
}

void unlock(volatile int * iter_lock){
	int expected = locked;
	if(!__atomic_compare_exchange(&iter_lock, &expected, unlocked, 0, __ATOMIC_ACQ_REL, __ATOMIC_ACQUIRE)){
		printf("Failed to unlock\n");
		return;
	}
}



5) 
obviously we only let one thread into the function at a time because we lock it before we call on the critical function
had we locked it within the critical function then we would've been able to have alternating read and writes
between the 4 threads

6)
I had to swap the values which unlocked and locked because in my workload.c code i had #define unlocked  0 
Had i had it the other way i would not have had to swap the values.


7)
void lock(){
	int ret_val;
	ret_val = __atomic_sub_fetch(&iter_lock, 1, __ATOMIC_ACQ_REL);
	while (ret_val<locked){
		syscall( SYS_futex, &iter_lock, FUTEX_WAIT, ret_val, NULL);
		ret_val = __atomic_sub_fetch(&iter_lock, 1, __ATOMIC_ACQ_REL);
	}
	return;
}

void unlock(){
	int ret_val;
	ret_val = __atomic_add_fetch(&iter_lock, 1, __ATOMIC_ACQ_REL );
	if (ret_val!=unlocked){
		__atomic_store_n(&iter_lock, 1, __ATOMIC_RELEASE );
		syscall( SYS_futex, &iter_lock, FUTEX_WAKE, INT_MAX );
	}
}


8) 
Just like the workload.c script, only one thread can access the function at a time because
we lock and unlock before and after the call to the function. 
This forces the thread to get stuck in the lock function right before entering 
the critical function. If we call on lock within the critical function then all threads
can be within the criticla function

9)
it is not possible to do a proper sleep lock entirely in userspace since 
the userspace processes or threads cannot put itself or others to sleep
we instead need the kernel to do this for the userspace. hence the syscall is necessary



