1) kenichi matsuo

2) it occupies all of the cores because we distribute all of the work among all 4 cores
since we parallelised the work. the work load is significant so 4 cores can be used to the max

3)
// 500 million iterations should take several seconds to run
#define ITERS 500000000
#define locked  1;
#define unlocked  0;
int iter_lock;

void critical_section( void ){
	int index = 0;
	while(index < ITERS){ index++; }
}

void lock(volatile int * iter_lock){
	printf("iter_lock: %d\n",*iter_lock);
}

void unlock(volatile int * iter_lock){

}

int main (int argc, char* argv[]){
	iter_lock = unlocked;
	// Create a team of threads on each processor
	#pragma omp parallel
	{
		// Each thread executes this code block independently
		lock(&iter_lock);
		critical_section();
		unlock(&iter_lock);




4)
void lock(volatile int * iter_lock){
	int expected = unlocked;
	while(!__atomic_compare_exchange(&iter_lock, &expected, locked, 0, __ATOMIC_ACQ_REL, __ATOMIC_ACQUIRE)){
		expected = unlocked;
	}
}

void unlock(volatile int * iter_lock){
	int expected = locked;
	if(!__atomic_compare_exchange(&iter_lock, &expected, unlocked, 0, __ATOMIC_ACQ_REL, __ATOMIC_ACQUIRE)){
		printf("Failed to unlock\n");
		return;
	}
}



5) 
obviously we only let one thread into the function at a time because we lock it before we call on the critical function
had we locked it within the critical function then we would've been able to have alternating read and writes
between the 4 threads



